# test_hrplus_transcribe.py
# 1) wav ê²½ë¡œ í•˜ë“œì½”ë”©
# 2) make_cqt_dataset.py ì™€ 100% ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ CQT ìƒì„±
# 3) HRPlus ëª¨ë¸ ë¡œë“œ í›„ ì¶”ë¡ 
# 4) onset/frame/offset ê¸°ë°˜ìœ¼ë¡œ ë…¸íŠ¸ ë””ì½”ë”©
# 5) MIDI ì €ì¥

import os
from pathlib import Path

import librosa
import numpy as np
import torch
import torch.nn.functional as F
import pretty_midi

from model import HRPlus   # ë„ˆì˜ HRPlus ëª¨ë¸

# ===========================
# í•˜ë“œì½”ë”© êµ¬ê°„
# ===========================

AUDIO_PATH = r"C:\Users\hyi8402\Desktop\CQT\ì´.mp3"
CKPT_PATH  = r"C:\Users\hyi8402\Desktop\Sound to Sheet\app\Transcription\2.result_hrplus\hrplus_best.pt"

OUTPUT_MIDI = os.path.splitext(AUDIO_PATH)[0] + "_hrplus.mid"

SR   = 16000
HOP  = 512
FRAME_TIME = HOP / SR
N_BINS = 88
FMIN = librosa.note_to_hz("A0")   # ë™ì¼í•œ fmin ì‚¬ìš©
MIDI_LOW = 21     # A0
MIDI_HIGH = 108   # C8

# ğŸ”¹ ì²­í¬ ê¸¸ì´ (ì´ˆë‹¨ìœ„)
CHUNK_SEC = 20.0   # 20ì´ˆì”© ìë¥´ê¸°

# ë””ì½”ë”© threshold (ê¸°ë³¸ê°’)
ONSET_TH      = 0.3
FRAME_ON_TH   = 0.25
FRAME_OFF_TH  = 0.15
MIN_DUR_SEC   = 0.03

# ğŸ”¹ í”¼ì¹˜ê°€ ë†’ì„ìˆ˜ë¡ onset ê¸°ì¤€ì„ ì–¼ë§ˆë‚˜ ì™„í™”í• ì§€ (ìµœëŒ€ ê°ì†ŒëŸ‰)
PITCH_RELAX_MAX = 0  # ë§¨ ìœ„ ìŒì—­ì€ ONSET_TH - 0.05ê¹Œì§€ í—ˆìš©


# ===========================
# 1) ì˜¤ë””ì˜¤ â†’ CQT (í•™ìŠµ CQTì™€ 100% ë™ì¼)
# ===========================
def audio_to_cqt_tensor(path: Path):
    y, sr = librosa.load(path.as_posix(), sr=SR, mono=True)

    cqt = librosa.cqt(
        y,
        sr=SR,
        hop_length=HOP,
        fmin=FMIN,
        n_bins=N_BINS,
        bins_per_octave=12
    )
    cqt_mag = np.abs(cqt).astype(np.float32)        # (n_bins, T)
    cqt_tensor = torch.from_numpy(cqt_mag).unsqueeze(0)  # (1, F, T)
    return cqt_tensor.unsqueeze(0)  # (1, 1, F, T)


# ===========================
# 2) ëª¨ë¸ ë¡œë“œ
# ===========================
def load_model(ckpt_path, device):
    model = HRPlus(
        n_pitches=88,
        cqt_bins=88,
        in_channels=1,
        base_channels=64,
        gru_hidden=128,
        pool_freq=1,   # í•™ìŠµê³¼ ë™ì¼
    ).to(device)

    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
    state_dict = ckpt["model"] if "model" in ckpt else ckpt
    model.load_state_dict(state_dict)
    model.eval()
    return model


# ===========================
# 3-A) í—¬í¼: local max / parabolic refinement
# ===========================
def is_local_max(col, t):
    """1D array colì—ì„œ tê°€ ì–‘ëì´ ì•„ë‹ˆê³  ì–‘ ì˜†ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ì§€"""
    if t <= 0 or t >= len(col) - 1:
        return False
    return (col[t] >= col[t - 1]) and (col[t] >= col[t + 1])


def refine_peak_time(col, t):
    """
    onset í™•ë¥  ì‹œí€€ìŠ¤ col (shape: (T,))ì—ì„œ
    t-1, t, t+1 ì„¸ ì ìœ¼ë¡œ 1D parabola í”¼íŒ…í•´ì„œ
    ì„œë¸Œí”„ë ˆì„ ì •ë°€ë„ t_refined ë°˜í™˜
    """
    T = len(col)
    if t <= 0 or t >= T - 1:
        return float(t)

    y1, y2, y3 = float(col[t - 1]), float(col[t]), float(col[t + 1])
    denom = (y1 - 2.0 * y2 + y3)
    if abs(denom) < 1e-8:
        return float(t)

    delta = 0.5 * (y1 - y3) / denom  # ë³´í†µ -0.5~+0.5 ê·¼ì²˜
    # ë„ˆë¬´ íŠ€ëŠ” ê²½ìš° ë°©ì§€
    if delta < -1.0:
        delta = -1.0
    elif delta > 1.0:
        delta = 1.0

    return float(t + delta)


# ===========================
# 3-B) ë…¸íŠ¸ ì¶”ì¶œ ë¡œì§ (HRìš© ë””ì½”ë”)
# ===========================
def extract_notes(onset_p, frame_p, on_th, fr_on_th, fr_off_th, min_dur):
    """
    onset_p:  (T, 88) sigmoidëœ í™•ë¥  (numpy)
    frame_p:  (T, 88)
    """
    T, K = onset_p.shape

    active_notes = {}   # pitch(k) -> onset_time(float)
    notes = []

    for t in range(T):
        # onset ê°ì§€
        for k in range(K):
            col = onset_p[:, k]  # íŠ¹ì • í”¼ì¹˜ k ì˜ ì „ì²´ ì‹œí€€ìŠ¤ (T,)

            # í”¼ì¹˜ë³„ ê°€ë³€ threshold: ê³ ìŒì¼ìˆ˜ë¡ ì¡°ê¸ˆ ë” ê´€ëŒ€
            midi_pitch = MIDI_LOW + k
            pitch_ratio = (midi_pitch - MIDI_LOW) / (MIDI_HIGH - MIDI_LOW)
            relax = PITCH_RELAX_MAX * pitch_ratio
            dyn_th = max(0.05, on_th - relax)  # ë„ˆë¬´ ë‚®ì•„ì§€ì§€ ì•Šê²Œ í•˜í•œì„ 

            # local max + threshold ì¡°ê±´ + ì´ë¯¸ ì¼œì§„ ìŒì€ ë‹¤ì‹œ ì‹œì‘ X
            if (
                onset_p[t, k] >= dyn_th
                and is_local_max(col, t)
                and k not in active_notes
            ):
                t_refined = refine_peak_time(col, t)  # íŒŒë¼ë³¼ë¦­ ë³´ì •
                onset_time = t_refined * FRAME_TIME
                active_notes[k] = onset_time

        # frame-based note ì¢…ë£Œ íŒì •
        for k in list(active_notes.keys()):
            p = frame_p[t, k]
            if p < fr_off_th:
                on_time = active_notes[k]
                off_time = t * FRAME_TIME

                if off_time - on_time >= min_dur:
                    notes.append((on_time, off_time, k))
                del active_notes[k]

    # ëì— ë‚¨ì€ ë…¸íŠ¸ ì •ë¦¬
    for k, on_time in active_notes.items():
        off_time = T * FRAME_TIME
        if off_time - on_time >= min_dur:
            notes.append((on_time, off_time, k))

    return notes


# ===========================
# 4) MIDIë¡œ ì €ì¥
# ===========================
def save_as_midi(notes, midi_path):
    pm = pretty_midi.PrettyMIDI()
    inst = pretty_midi.Instrument(program=0)

    for on, off, k in notes:
        pitch = MIDI_LOW + k
        n = pretty_midi.Note(
            velocity=90,
            pitch=pitch,
            start=on,
            end=off
        )
        inst.notes.append(n)

    pm.instruments.append(inst)
    pm.write(midi_path)
    print(f"[MIDI SAVED] {midi_path}")


# ===========================
# MAIN (ì²­í¬ ë‹¨ìœ„ ì¶”ë¡ )
# ===========================
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using:", device)

    # ì „ì²´ CQT
    feat = audio_to_cqt_tensor(Path(AUDIO_PATH)).to(device)  # (1,1,88,T_total)
    print("CQT shape:", feat.shape)
    _, _, _, T_total = feat.shape

    # ì²­í¬ ë‹¹ í”„ë ˆì„ ìˆ˜
    chunk_frames = int(CHUNK_SEC / FRAME_TIME)
    print(f"Chunk frames: {chunk_frames} (â‰ˆ {CHUNK_SEC:.1f} sec)")

    # ëª¨ë¸
    model = load_model(CKPT_PATH, device)

    all_notes = []

    with torch.no_grad():
        start_frame = 0
        while start_frame < T_total:
            end_frame = min(start_frame + chunk_frames, T_total)
            feat_chunk = feat[:, :, :, start_frame:end_frame]  # (1,1,88,T_chunk)
            T_chunk = end_frame - start_frame

            if T_chunk <= 0:
                break

            print(f"Processing frames {start_frame} ~ {end_frame} (T_chunk={T_chunk})")

            out = model(feat_chunk)
            onset_p = torch.sigmoid(out["onset_logits"])[0].cpu().numpy()  # (T_chunk, 88)
            frame_p = torch.sigmoid(out["frame_logits"])[0].cpu().numpy()

            # ì´ chunk ë‚´ë¶€(0~T_chunk*FRAME_TIME ê¸°ì¤€)ì—ì„œ ë…¸íŠ¸ ì¶”ì¶œ
            notes_chunk = extract_notes(
                onset_p, frame_p,
                ONSET_TH, FRAME_ON_TH, FRAME_OFF_TH,
                MIN_DUR_SEC
            )

            # chunk ì‹œì‘ ì‹œê°„(sec)ë§Œí¼ ë”í•´ì„œ ì „ì²´ íƒ€ì„ìœ¼ë¡œ ì´ë™
            chunk_start_sec = start_frame * FRAME_TIME
            for on, off, k in notes_chunk:
                all_notes.append((on + chunk_start_sec, off + chunk_start_sec, k))

            start_frame = end_frame

    print(f"Total detected notes: {len(all_notes)}")

    # MIDI ì €ì¥
    save_as_midi(all_notes, OUTPUT_MIDI)


if __name__ == "__main__":
    main()
